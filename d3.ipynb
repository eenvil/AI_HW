{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steam Reviews Classification Notebook\n",
    "\n",
    "This notebook demonstrates several methods to classify Steam reviews using different approaches:\n",
    "\n",
    "- **Traditional ML:** Random Forest, XGBoost, and Logistic Regression with TF-IDF (using n-grams).\n",
    "- **Transformer-based:** DistilBERT (via a sentiment analysis pipeline) and Zero-Shot Classification with BART.\n",
    "\n",
    "The target variable is the review sentiment (from the `voted_up` field, converted to binary labels: 1 for positive, 0 for negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\code\\python\\TMP\\.conda\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Happy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Traditional ML classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Transformer-based methods from Hugging Face\n",
    "from transformers import pipeline\n",
    "\n",
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_id</th>\n",
       "      <th>review</th>\n",
       "      <th>voted_up</th>\n",
       "      <th>is_english</th>\n",
       "      <th>cleaned_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>620</td>\n",
       "      <td>I don’t remember signing up for this. Maybe I ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>’ remember signing . Maybe . idea long . Maybe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>620</td>\n",
       "      <td>I want to say this, i dont think valve is mak...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>want say , dont think valve making portal 3 . ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>620</td>\n",
       "      <td>Ah, Portal 2. The portal gun is iconic. When p...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Ah , Portal 2 . portal gun iconic . platformin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>620</td>\n",
       "      <td>Portal 2 is one game I hesitated to get it but...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>Portal 2 one game hesitated get believe review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>620</td>\n",
       "      <td>this gotta one of my favorite games to play in...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>got ta one favorite games play free time fun g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   app_id                                             review  voted_up  \\\n",
       "0     620  I don’t remember signing up for this. Maybe I ...      True   \n",
       "1     620   I want to say this, i dont think valve is mak...      True   \n",
       "2     620  Ah, Portal 2. The portal gun is iconic. When p...      True   \n",
       "3     620  Portal 2 is one game I hesitated to get it but...      True   \n",
       "4     620  this gotta one of my favorite games to play in...      True   \n",
       "\n",
       "   is_english                                     cleaned_review  \n",
       "0        True  ’ remember signing . Maybe . idea long . Maybe...  \n",
       "1        True  want say , dont think valve making portal 3 . ...  \n",
       "2        True  Ah , Portal 2 . portal gun iconic . platformin...  \n",
       "3        True  Portal 2 one game hesitated get believe review...  \n",
       "4        True  got ta one favorite games play free time fun g...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the cleaned data (make sure 'steam_reviews_cleaned.csv' is in your working directory)\n",
    "df = pd.read_csv('steam_reviews_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 3077\n",
      "Test set size: 770\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data for classification\n",
    "\n",
    "# The target is the 'voted_up' column (convert it to integer: 1 for positive, 0 for negative)\n",
    "X = df['cleaned_review']\n",
    "y = df['voted_up'].astype(int)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print('Training set size:', len(X_train))\n",
    "print('Test set size:', len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF features shape: (3077, 81912)\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text using TF-IDF with n-grams (unigrams and bigrams)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "print('TF-IDF features shape:', X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8480519480519481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.21      0.34       140\n",
      "           1       0.85      0.99      0.91       630\n",
      "\n",
      "    accuracy                           0.85       770\n",
      "   macro avg       0.83      0.60      0.63       770\n",
      "weighted avg       0.84      0.85      0.81       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "pred_rf = rf.predict(X_test_tfidf)\n",
    "print('Random Forest Accuracy:', accuracy_score(y_test, pred_rf))\n",
    "print(classification_report(y_test, pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\code\\python\\TMP\\.conda\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [05:12:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.44      0.56       140\n",
      "           1       0.89      0.97      0.92       630\n",
      "\n",
      "    accuracy                           0.87       770\n",
      "   macro avg       0.82      0.70      0.74       770\n",
      "weighted avg       0.86      0.87      0.86       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train an XGBoost classifier\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "pred_xgb = xgb.predict(X_test_tfidf)\n",
    "print('XGBoost Accuracy:', accuracy_score(y_test, pred_xgb))\n",
    "print(classification_report(y_test, pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with K-grams\n",
    "\n",
    "Here we use a Logistic Regression classifier with the same TF-IDF features (using n-grams) to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression (K-grams) Accuracy: 0.8558441558441559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.22      0.36       140\n",
      "           1       0.85      1.00      0.92       630\n",
      "\n",
      "    accuracy                           0.86       770\n",
      "   macro avg       0.90      0.61      0.64       770\n",
      "weighted avg       0.87      0.86      0.82       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistic Regression classifier\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict and evaluate on the test set\n",
    "pred_lr = lr.predict(X_test_tfidf)\n",
    "print('Logistic Regression (K-grams) Accuracy:', accuracy_score(y_test, pred_lr))\n",
    "print(classification_report(y_test, pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT Classification\n",
    "\n",
    "Here we use a pre-built Hugging Face sentiment-analysis pipeline with DistilBERT. Note that the model (`distilbert-base-uncased-finetuned-sst-2-english`) is fine-tuned on the SST-2 dataset, so its predictions may not perfectly align with Steam reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\code\\python\\TMP\\.conda\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From e:\\code\\python\\TMP\\.conda\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Accuracy: 0.8207792207792208\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.86      0.63       140\n",
      "           1       0.96      0.81      0.88       630\n",
      "\n",
      "    accuracy                           0.82       770\n",
      "   macro avg       0.73      0.83      0.76       770\n",
      "weighted avg       0.88      0.82      0.84       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DistilBERT sentiment analysis pipeline\n",
    "distilbert_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Function to obtain predictions from DistilBERT\n",
    "def get_distilbert_predictions(reviews):\n",
    "    preds = []\n",
    "    for review in reviews:\n",
    "        # Pass truncation=True so that texts longer than 512 tokens are truncated\n",
    "        result = distilbert_classifier(review, truncation=True)[0]\n",
    "        # Convert the label to a binary value: 1 for POSITIVE, 0 for NEGATIVE\n",
    "        pred = 1 if result['label'] == \"POSITIVE\" else 0\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Get predictions on the test set (this may take a while for a large dataset)\n",
    "pred_distilbert = get_distilbert_predictions(X_test.tolist())\n",
    "\n",
    "print('DistilBERT Accuracy:', accuracy_score(y_test, pred_distilbert))\n",
    "print(classification_report(y_test, pred_distilbert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Decoder (Zero-Shot Classification with BART)\n",
    "\n",
    "For a transformer decoder approach, we demonstrate zero-shot classification using Facebook's BART model. Zero-shot classification allows us to classify texts without a task-specific fine-tuning, making it a flexible (albeit computationally heavy) option.\n",
    "\n",
    "Here we define candidate labels for sentiment as `positive` and `negative`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Decoder (BART) Zero-Shot Accuracy: 0.825974025974026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.92      0.66       140\n",
      "           1       0.98      0.80      0.88       630\n",
      "\n",
      "    accuracy                           0.83       770\n",
      "   macro avg       0.75      0.86      0.77       770\n",
      "weighted avg       0.89      0.83      0.84       770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Use \"roberta-large-mnli\" for zero-shot classification\n",
    "# !!!Put your your own Access Token to WORK !!!\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    token=\"\" #HERE\n",
    ")\n",
    "\n",
    "# Define candidate labels for sentiment\n",
    "candidate_labels = [\"negative\", \"positive\"]\n",
    "\n",
    "# Function to obtain predictions using zero-shot classification\n",
    "def get_bart_predictions(reviews):\n",
    "    preds = []\n",
    "    for review in reviews:\n",
    "        result = zero_shot_classifier(review, candidate_labels)\n",
    "        # The label with the highest score is used as the prediction\n",
    "        pred_label = result['labels'][0]\n",
    "        pred = 1 if pred_label.lower() == \"positive\" else 0\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Get predictions on the test set (again, this may be slow on a large dataset)\n",
    "pred_bart = get_bart_predictions(X_test.tolist())\n",
    "\n",
    "print('Transformer Decoder (BART) Zero-Shot Accuracy:', accuracy_score(y_test, pred_bart))\n",
    "print(classification_report(y_test, pred_bart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook explored several approaches for classifying Steam reviews. You can compare the performance of traditional ML methods (using TF-IDF and n-grams) with modern transformer-based methods. Depending on your computational resources and the specifics of your dataset, you may choose to further fine-tune or extend these models for improved performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
